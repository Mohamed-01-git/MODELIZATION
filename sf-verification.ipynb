{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d88ed3b",
   "metadata": {},
   "source": [
    "![logo](./img/LogoLine_horizon_C3S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d236c61",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5dbf9e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# Seasonal Forecast Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2c373",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "description"
    ]
   },
   "source": [
    "This notebook provides a practical introduction on how to produce some verification metrics and scores for seasonal forecasts with data from the [Copernicus Climate Change Service (C3S)](https://climate.copernicus.eu/). C3S seasonal forecast products are based on data from several state-of-the-art seasonal prediction systems. In this notebook, as an example, we will focus on data produced by [CMCC SPSv3.5 system](https://confluence.ecmwf.int/display/CKB/Description+of+CMCC-CM2-v20191201+C3S+contribution), which is one of the forecasting systems available through C3S.\n",
    "\n",
    "The tutorial will demonstrate how to access retrospective forecast (hindcast) data of 2-metre temperature initialized in the period 1993-2016, with a forecast start date in the 1st of March. All these forecasts are 6 months long (from March to August). More details about the role of the hindcasts can be found in [this Copernicus Knowledge Base article](https://confluence.ecmwf.int/display/CKB/Seasonal+forecasts+and+the+Copernicus+Climate+Change+Service). Observation data (ERA5 reanalysis) for the same reference period and the same months will also be obtained from the CDS. The tutorial will then show how to compute some deterministic products (anomalies) and some probabilistic products (probabilities for tercile categories). In addition to the 1-month average data retrieved from the CDS, 3-months aggregations will be also produced. Finally, verification metrics (correlation, area under the ROC curve, and RPS) will be calculated and visualised in a set of plots.\n",
    "\n",
    "Please see here the full documentation of the [C3S Seasonal Forecast Datasets](https://confluence.ecmwf.int/display/CKB/C3S+Seasonal+Forecasts%3A+datasets+documentation). This notebook will use data from the CDS dataset [seasonal forecast monthly statistics on single levels](https://cds.climate.copernicus.eu/cdsapp#!/dataset/seasonal-monthly-single-levels?tab=overview) (as opposed to multiple levels in the atmosphere)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e33e2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>NOTE</b>: <br>\n",
    "    An example on the use of the code of this notebook to create verification plots for the operational seasonal forecast systems available in the CDS can be found in a <a href=\"https://confluence.ecmwf.int/display/CKB/C3S+seasonal+forecasts+verification+plots\">dedicated page</a> of the documentation hosted in the Copernicus Knowledge Base.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f91fff-9357-4fd6-a13c-38934d81efae",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252754c-bf4e-4aa8-a849-77ad00471338",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "learning-objectives"
    ]
   },
   "source": [
    "## Learning objectives üß†\n",
    "\n",
    "1. Learn how to calculate monthly and 3-monthly anomalies.\n",
    "\n",
    "2. Learn how to compute probabilities for tercile categories.\n",
    "\n",
    "3. Learn to calculate and visualize the verification metrics correlation, area under the ROC curve and RPS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a3428-3553-4ed0-9916-bb71e67d054c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8fa7bd-4847-4ce8-b329-c563eac9a5eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "target-audience"
    ]
   },
   "source": [
    "## Target audience üéØ\n",
    "\n",
    "**Anyone** interested in learning how to calculate seasonal forecast anomalies and to make seasonal forecast verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee2165-fa97-474d-bf49-8cd5aa751032",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323a373-ebc6-4250-aacb-6decd817bae8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "prerequisites"
    ]
   },
   "source": [
    "## Prerequisites and assumed knowledge üîç\n",
    "1. **Programming Skills**: Familiarity with programming concepts, particularly in Python, as the tutorial involves using loops, defining functions and using multiple different libraries for data manipulation and visualization.\n",
    "   \n",
    "2. **Familiarity with API Usage**: Understanding of how to use Application Programming Interfaces (APIs) will be useful for accessing data through the CDS API.\n",
    "\n",
    "3. **Familiarity with multidimensional data structure**: Comprehending what are data dimensions and how they are organized in an array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428e2c4-bea1-4be6-a433-4aefd1c74729",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca905044-9c01-4ce6-b125-166bc08fab1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "difficulty"
    ]
   },
   "source": [
    ":::{admonition} Difficulty\n",
    ":class: tip\n",
    "4/5\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050d3e5-b484-42b4-8df6-edd6b4900dd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5ecc5-d354-433d-92b2-c0a2358374d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "run-the-tutorial"
    ]
   },
   "source": [
    ":::{dropdown} Run the tutorial\n",
    ":open:\n",
    "### WEKEO\n",
    "\n",
    "[WEkEO](https://www.wekeo.eu/) serves as the official platform of the European Centre for Medium-Range Weather Forecasts (ECMWF), offering access to an extensive array of climate data and tools for analysis and visualization. It provides a robust environment for conducting in-depth analysis and exploration of climate-related datasets. To learn more about WEkEO and its offerings, visit their [website](https://www.wekeo.eu/).\n",
    "\n",
    "[Run this notebook on WEKEO](https://www.wekeo.eu/)\n",
    "\n",
    "### Possible Cloud Services\n",
    "\n",
    "While Kaggle, Binder, and Colab are popular options for running notebooks in the cloud, it's essential to note that these are just a few among many available choices. Each platform has its unique features and capabilities, catering to diverse user needs and preferences.\n",
    "\n",
    "\n",
    "\n",
    "| **Kaggle** | **Binder** | **Colab** |\n",
    "|:----------:|:---------:|:--------:|\n",
    "| [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Randbee/sf-verification/blob/master/sf-verification.ipynb) | [![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/Randbee/sf-verification/master?labpath=sf-verification.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Randbee/sf-verification/blob/master/sf-verification.ipynb) |\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa7b16-4e8c-4b8a-80c2-bfc5b4e8aa2a",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07976f3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "outline"
    ]
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Import the necessary libraries\n",
    "2. Access data with the CDS API\n",
    "    1. Retrieve hindcast data\n",
    "    2. Retrieve observations data (ERA5)\n",
    "3. Compute deterministic and probabilistic products from the hindcast data\n",
    "    1. Anomalies\n",
    "    2. Probabilities for tercile categories\n",
    "4. Compute deterministic and probabilistic scores\n",
    "    1. Read observations data into a xr.Dataset\n",
    "    2. Compute deterministic scores\n",
    "    3. Compute probabilistic scores for tercile categories\n",
    "5. Visualize verification plots\n",
    "    1. Correlation\n",
    "    2. Ranked Probability Score (RPS)\n",
    "    3. Area under Relative Operating Characteristic (ROC) curve\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596a877",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>WARNING</b>: <br>\n",
    "    Please take into account that to run this tutorial, around 8GB of RAM and around 5GB of disk space are needed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f369bd-5214-417a-8d8d-b365125033ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a551fed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f14f30",
   "metadata": {},
   "source": [
    "For downloading the data from the CDS we will need [cdsapi](https://pypi.org/project/cdsapi/). We will be working with data in GRIB format, which we will need to map to the NetCDF Common Data Model. For that, we will need [cfgrib](https://github.com/ecmwf/cfgrib). To handle NetCDF, we will use [xarray](https://docs.xarray.dev/en/stable/), and to deal with dataframes, [pandas](https://pandas.pydata.org/). We will also need libraries for plotting and viewing data, in this case we will use [matplotlib](https://matplotlib.org/) and [cartopy](https://scitools.org.uk/cartopy/docs/latest/). [xskillscore](https://xskillscore.readthedocs.io/en/stable/) will be used to calculate the verification metrics. <br>\n",
    "If you don't have any of the following libraries, you must install it. You can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b692d9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CDS API\n",
    "import cdsapi\n",
    "\n",
    "import cfgrib\n",
    "# Libraries for working with multi-dimensional arrays\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Forecast verification metrics with xarray\n",
    "import xskillscore as xs\n",
    "\n",
    "# Date and calendar libraries\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "\n",
    "# Libraries for plotting and geospatial data visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Disable warnings for data download via API and matplotlib (do I need both???)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f26c3-a795-46ea-a2e8-9df4f3b32ebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf201000",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Access data with the CDS API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f7910",
   "metadata": {},
   "source": [
    "First we will set up some common variables to be used in this notebook to define the folder containing the data, the variables to be retrieved, the hindcast period and the [nominal start month](https://confluence.ecmwf.int/display/CKB/Summary+of+available+data#Summaryofavailabledata-nominal_start_date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e72ed409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATADIR = '/home/mohamed/EHTPIII/MODELISATION/DATA'\n",
    "\n",
    "config = dict(\n",
    "    list_vars = ['2m_temperature', ],\n",
    "    hcstarty = 1993,\n",
    "    hcendy = 2016,\n",
    "    start_month = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd62346-ba37-4e46-99bd-fd3f0d60c154",
   "metadata": {},
   "source": [
    "After setting up this initial configuration variables, the existence of all the data folders will be checked and directories will be created if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f09c485-88ae-4307-acba-cd4cfb6a8f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SCOREDIR = DATADIR + '/scores'\n",
    "PLOTSDIR = DATADIR + f'/plots/stmonth{config[\"start_month\"]:02d}'\n",
    "\n",
    "for directory in [DATADIR, SCOREDIR, PLOTSDIR]:\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        # If it doesn't exist, create it\n",
    "        os.makedirs(directory)\n",
    "        print(f'Creating folder {directory}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1220b",
   "metadata": {},
   "source": [
    "To access data from the CDS, you will need first to [register](https://cds.climate.copernicus.eu/#!/home) at the ADS registration page (if you haven't already done so), [log in](https://cds.climate.copernicus.eu/user/login), and accept the Terms and Conditions at the end of the `Download data` tab.<br>\n",
    "\n",
    "To obtain data programmatically from the CDS, you will need an API Key that can be found in the [api-how-to page](https://cds.climate.copernicus.eu/api-how-to). Here your key will appear automatically in the black window, assuming you have already registered and logged into the ADS. Your API key is the entire string of characters that appears after `key:`\n",
    "\n",
    "Now copy your API key into the code cell below, replacing `#######` with your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2954b6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 12:28:42,701 WARNING [2024-10-15T00:00:00] System is having technical problems. We are working to resume operations asap. You can check status [here](https://status.ecmwf.int/).\n",
      "2024-10-17 12:28:42,703 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-10-17 12:28:42,705 WARNING [2024-09-26T00:00:00] Should you have not yet migrated from the old CDS system to the new CDS, please check our [informative page](https://confluence.ecmwf.int/x/uINmFw) for guidance.\n",
      "2024-10-17 12:28:42,706 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-10-17 12:28:42,707 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-10-17 12:28:42,708 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    }
   ],
   "source": [
    "CDSAPI_URL = 'https://cds-beta.climate.copernicus.eu/api'\n",
    "CDSAPI_KEY = '328ae3b8-36e2-4c27-9892-c7a55f386cdf'\n",
    "c = cdsapi.Client(url=CDSAPI_URL, key=CDSAPI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a0c4b-dc30-4227-844f-5bb74aa21d70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a8b90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.A. Retrieve hindcast data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1476d6e",
   "metadata": {
    "papermill": {
     "duration": 0.42258,
     "end_time": "2022-03-14T17:52:13.01783",
     "exception": false,
     "start_time": "2022-03-14T17:52:12.59525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The next step is then to request the seasonal forecast monthly statistics data on single levels with the help of the CDS API. Below, we download a GRIB file containing the retrospective forecasts (hindcasts, or reforecasts) for the variables, start month and years defined in the variable `config` \n",
    "\n",
    "Running the code block below will download the data from the CDS as specified by the following API keywords:\n",
    "\n",
    "> **Format**: `Grib` <br>\n",
    "> **Originating centre**: `CMCC` <br>\n",
    "> **System**: `35` *this refers to CMCC SPSv3.5* <br>\n",
    "> **Variable**: `2-metre temperature` <br>\n",
    "> **Product type**: `Monthly mean` *all ensemble members will be retrieved* <br>\n",
    "> **Year**: `1993 to 2016`  <br>\n",
    "> **Month**: `03` *March* <br>\n",
    "> **Leadtime month**: `1 to 6` *All available lead times (March to August)*\n",
    "\n",
    "Note we will also be defining some variable, `origin`, with the forecast system details to be included in the `config` variable. Additionally it defines a base name variable `hcst_bname` to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530dce10",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>NOTE</b>: <br>\n",
    "    The API request below can be generated automatically from the <a href=\"https://cds.climate.copernicus.eu/cdsapp#!/dataset/seasonal-monthly-single-levels?tab=form\">CDS download page</a>. At the end of the download form there is a <code>Show API request</code> icon, which allows a copy-paste of the code below. At this point, remember to accept the Terms and Conditions!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d76fa91f-8f16-437f-8c86-c8cc64323719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohamed/EHTPIII/MODELISATION/DATA/cmcc_s35_stmonth03_hindcast1993-2016_monthly.grib\n"
     ]
    }
   ],
   "source": [
    "origin = 'cmcc.s35'\n",
    "origin_labels = {'institution': 'CMCC', 'name': 'SPSv3.5'}\n",
    "\n",
    "config['origin'], config['system'] = origin.split('.s')\n",
    "\n",
    "\n",
    "hcst_bname = '{origin}_s{system}_stmonth{start_month:02d}_hindcast{hcstarty}-{hcendy}_monthly'.format(**config)\n",
    "hcst_fname = f'{DATADIR}/{hcst_bname}.grib'\n",
    "print(hcst_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbae2aaf-6356-49e8-a515-77316e0ab260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 12:11:19,740 INFO Request ID is e713155f-d3d0-4d84-a237-f108a1ff7e4e\n",
      "2024-10-17 12:11:20,254 INFO status has been updated to accepted\n",
      "2024-10-17 12:11:27,869 INFO status has been updated to successful\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/mohamed/EHTPIII/MODELISATION/DATA/cmcc_s35_stmonth03_hindcast1993-2016_monthly.grib'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.retrieve(\n",
    "    'seasonal-monthly-single-levels',\n",
    "    {\n",
    "        'format': 'grib',\n",
    "        'originating_centre': config['origin'],\n",
    "        'system': config['system'],\n",
    "        'variable': config['list_vars'],\n",
    "        'product_type': 'monthly_mean',\n",
    "        'year': ['{}'.format(yy) for yy in range(config['hcstarty'],config['hcendy']+1)],\n",
    "        'month': '{:02d}'.format(config['start_month']),\n",
    "        'leadtime_month': ['1', '2', '3','4', '5', '6'],\n",
    "    },\n",
    "    hcst_fname)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15876866-bdb5-462a-97ee-381634324f7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fff68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.B. Retrieve observations data (ERA5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d34806",
   "metadata": {
    "papermill": {
     "duration": 0.42258,
     "end_time": "2022-03-14T17:52:13.01783",
     "exception": false,
     "start_time": "2022-03-14T17:52:12.59525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we will request from the CDS the observation data that will be used as the ground truth against which the hindcast data will be compared. In this notebook we will be using as observational reference the CDS dataset `reanalysis-era5-single-levels-monthly-means` which contains [ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels-monthly-means?tab=overview) monthly averaged data from the CDS. In order to compare it with the hindcast data we have just downloaded above, we will ask the CDS to regrid it to the same grid used by the C3S seasonal forecasts.\n",
    "\n",
    "Running the code block below will download the ERA5 data from the CDS as specified by the following API keywords:\n",
    "\n",
    "> **Format**: `Grib` <br>\n",
    "> **Variable**: `2-metre temperature` <br>\n",
    "> **Product type**: `Monthly averaged reanalysis`  <br>\n",
    "> **Year**: `1993 to 2016`  <br>\n",
    "> **Month**: `March to August` *same months contained in the hindcast data*<br>\n",
    "> **Time**: `00` <br>\n",
    "> **Grid**: `1x1-degree lat-lon regular grid` <br>\n",
    "> **Area**: `-89.5 to 89.5 in latitude; 0.5 to 359.5 in longitude` <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a332601-d145-40d6-a08c-1587072a90f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohamed/EHTPIII/MODELISATION/DATA/era5_monthly_stmonth03_1993-2016.grib\n"
     ]
    }
   ],
   "source": [
    "obs_fname = '{fpath}/era5_monthly_stmonth{start_month:02d}_{hcstarty}-{hcendy}.grib'.format(fpath=DATADIR,**config)\n",
    "print(obs_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592f568-a1f3-4821-b3bf-3112a0216341",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohamed/EHTPIII/MODELISATION/DATA/era5_monthly_stmonth03_1993-2016.grib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 12:15:41,028 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T in July 2024 - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-10-17 12:15:41,028 INFO Request ID is 49c29db2-7607-4886-889b-e901751fe8b5\n",
      "2024-10-17 12:15:41,297 INFO status has been updated to accepted\n",
      "2024-10-17 12:16:57,585 INFO status has been updated to successful\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/mohamed/EHTPIII/MODELISATION/DATA/era5_monthly_stmonth03_1993-2016.grib'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.retrieve(\n",
    "    'reanalysis-era5-single-levels-monthly-means',\n",
    "    {\n",
    "        'product_type': 'monthly_averaged_reanalysis',\n",
    "        'variable': config['list_vars'],\n",
    "        # NOTE from observations we need to go one year beyond so we have available all the right valid dates\n",
    "        # e.g. Nov.2016 start date forecast goes up to April 2017             \n",
    "        'year': ['{}'.format(yy) for yy in range(config['hcstarty'],config['hcendy']+2)],\n",
    "        'month': ['{:02d}'.format((config['start_month']+leadm)%12) if config['start_month']+leadm!=12 else '12' for leadm in range(6)],\n",
    "        'time': '00:00',\n",
    "        # We can ask CDS to interpolate ERA5 to the same grid used by C3S seasonal forecasts\n",
    "        'grid': '1/1',\n",
    "        'area': '89.5/0.5/-89.5/359.5',\n",
    "        'format': 'grib',\n",
    "    },\n",
    "    obs_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d128eb-3436-45aa-a70b-4cd0dca3691d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e09ca3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. Compute deterministic and probabilistic products from the hindcast data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3f5d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this section we will be calculating the different derived products we will be scoring later in the notebook. We will use the reference period defined in `config[\"hcstarty\"]`,`config[\"hcendy\"]` to calculate:\n",
    "* anomalies: defined as the difference of a given hindcast to the model climate, computed as the average over all the available members (that is, among all start years and ensemble members)\n",
    "* probabilities for tercile categories: defined as the proportion of members in a given hindcast lying within each one of the categories bounded by the terciles computed from all available members in the reference period\n",
    "\n",
    "In addition to the 1-month anomalies and probabilities, a rolling average of 3 months will be also computed to produce anomalies and probabilities for this 3-months aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c79b8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>NOTE</b>: <br>\n",
    "   Some of the seasonal forecast monthly data on the CDS comes from systems using members initialized on different start dates (lagged start date ensembles). In the GRIB encoding used for those systems we will therefore have two different <code>xarray/cfgrib</code> keywords for the real start date of each member (<code>time</code>) and for the nominal start date (<code>indexing_time</code>) which is the one we would need to use for those systems initializing their members with a lagged start date approach.\n",
    "    The following line of code will take care of that as long as we include the value <code>config['isLagged']=True</code> in the config dictionary as defined in section 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f022052",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the re-shaping of time coordinates in xarray.Dataset we need to select the right one \n",
    "#  -> burst mode ensembles (e.g. ECMWF SEAS5) use \"time\". This is the default option in this notebook\n",
    "#  -> lagged start ensembles (e.g. MetOffice GloSea6) use \"indexing_time\" (see CDS documentation about nominal start date)\n",
    "st_dim_name = 'time' if not config.get('isLagged',False) else 'indexing_time'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b2421-c6aa-4adb-8c18-23d469bacd29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447eff8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.A. Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fe064",
   "metadata": {},
   "source": [
    "We will start opening the hindcast data GRIB file we downloaded in the previous section to load it into an `xarray.Dataset` object.\n",
    "\n",
    "Some minor modifications of the `xr.Dataset` object also happen here:\n",
    "* We define chunks on the forecastMonth (leadtime) coordinate so `dask.array` can be used behind the scenes.\n",
    "* We rename some coordinates to align them with the names they will have in the observations `xr.Dataset` and those expected by default in the `xskillscore` package we will be using for the calculation of scores.\n",
    "* We create a new array named `valid_time` to be a new coordinate (depending on both `start_date` and `forecastMonth`).\n",
    "\n",
    "After that, both anomalies for 1-month and 3-months aggregations are computed and stored into a netCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f37f01-7433-4ad8-b5cc-75ea6ebfee87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading HCST data from file\n",
    "hcst = xr.open_dataset(hcst_fname,engine='cfgrib', backend_kwargs=dict(time_dims=('forecastMonth', st_dim_name)))\n",
    "hcst = hcst.chunk({'forecastMonth':1, 'latitude':'auto', 'longitude':'auto'})  #force dask.array using chunks on leadtime, latitude and longitude coordinate\n",
    "hcst = hcst.rename({'latitude':'lat','longitude':'lon', st_dim_name:'start_date'})\n",
    "\n",
    "# Re-arranging time metadata in xr.Dataset object\n",
    "\n",
    "## Add start_month to the xr.Dataset\n",
    "start_month = pd.to_datetime(hcst.start_date.values[0]).month\n",
    "hcst = hcst.assign_coords({'start_month':start_month})\n",
    "\n",
    "## Add valid_time to the xr.Dataset\n",
    "vt = xr.DataArray(dims=('start_date','forecastMonth'), coords={'forecastMonth':hcst.forecastMonth,'start_date':hcst.start_date})\n",
    "vt.data = [[pd.to_datetime(std)+relativedelta(months=fcmonth-1) for fcmonth in vt.forecastMonth.values] for std in vt.start_date.values]\n",
    "hcst = hcst.assign_coords(valid_time=vt)\n",
    "\n",
    "# CALCULATE 3-month AGGREGATIONS\n",
    "# NOTE rolling() assigns the label to the end of the N month period, so the first N-1 elements have NaN and can be dropped\n",
    "hcst_3m = hcst.rolling(forecastMonth=3).mean()\n",
    "hcst_3m = hcst_3m.where(hcst_3m.forecastMonth>=3,drop=True)\n",
    "\n",
    "# CALCULATE ANOMALIES (and save to file)\n",
    "## Computing anomalies 1m\n",
    "hcmean = hcst.mean(['number','start_date'])\n",
    "anom = hcst - hcmean\n",
    "anom = anom.assign_attrs(reference_period='{hcstarty}-{hcendy}'.format(**config))\n",
    "\n",
    "## Computing anomalies 3m\n",
    "hcmean_3m = hcst_3m.mean(['number','start_date'])\n",
    "anom_3m = hcst_3m - hcmean_3m\n",
    "anom_3m = anom_3m.assign_attrs(reference_period='{hcstarty}-{hcendy}'.format(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b20233-b95c-4566-9035-15987df0a8a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Saving anomalies 1m/3m to netCDF files\n",
    "anom.to_netcdf(f'{DATADIR}/{hcst_bname}.1m.anom.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed46e6-245c-4e15-9f28-7747fdb0c6bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "anom_3m.to_netcdf(f'{DATADIR}/{hcst_bname}.3m.anom.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfdefe-41ae-4919-bc85-fa43ed7b90a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bf201",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.B. Probabilities for tercile categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f1ece",
   "metadata": {},
   "source": [
    "Before we start computing the probabilities it will be useful to create a function devoted to computing the boundaries of a given category (`icat`) coming from a given set of quantiles (`quantiles`) of an `xr.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faece60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a function to calculate the boundaries of forecast categories defined by quantiles\n",
    "def get_thresh(icat,quantiles,xrds,dims=['number','start_date']):\n",
    "\n",
    "    if not all(elem in xrds.dims for elem in dims):           \n",
    "        raise Exception('Some of the dimensions in {} is not present in the xr.Dataset {}'.format(dims,xrds)) \n",
    "    else:\n",
    "        if icat == 0:\n",
    "            xrds_lo = -np.inf\n",
    "            xrds_hi = xrds.quantile(quantiles[icat],dim=dims)      \n",
    "            \n",
    "        elif icat == len(quantiles):\n",
    "            xrds_lo = xrds.quantile(quantiles[icat-1],dim=dims)\n",
    "            xrds_hi = np.inf\n",
    "            \n",
    "        else:\n",
    "            xrds_lo = xrds.quantile(quantiles[icat-1],dim=dims)\n",
    "            xrds_hi = xrds.quantile(quantiles[icat],dim=dims)\n",
    "      \n",
    "    return xrds_lo,xrds_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706afa29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will start by creating a list with the quantiles which will be the boundaries for our categorical forecast. In this example our product will be based on 3 categories ('below lower tercile', 'normal' and 'above upper tercile') so we will be using as category boundaries the values `quantiles = [1/3., 2/3.]`\n",
    "\n",
    "The `xr.Dataset` objects `hcst` and `hcst_3m` read in the previous step from the corresponding netCDF files will be used here, and therefore probabilities for both 1-month and 3-months aggregations are computed and stored into a netCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a7262",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  CALCULATE PROBABILITIES for tercile categories by counting members within each category\n",
    "\n",
    "quantiles = [1/3., 2/3.]\n",
    "numcategories = len(quantiles)+1\n",
    "\n",
    "for aggr,h in [(\"1m\",hcst), (\"3m\",hcst_3m)]:\n",
    "    # Computing tercile probabilities\n",
    "\n",
    "    l_probs_hcst=list()\n",
    "    for icat in range(numcategories):\n",
    "        h_lo,h_hi = get_thresh(icat, quantiles, h)\n",
    "        probh = np.logical_and(h>h_lo, h<=h_hi).sum('number')/float(h.dims['number'])\n",
    "        # Instead of using the coordinate 'quantile' coming from the hindcast xr.Dataset\n",
    "        # we will create a new coordinate called 'category'\n",
    "        if 'quantile' in probh:\n",
    "            probh = probh.drop('quantile')\n",
    "        l_probs_hcst.append(probh.assign_coords({'category':icat}))\n",
    "\n",
    "    # Concatenating tercile probs categories\n",
    "    probs = xr.concat(l_probs_hcst,dim='category')                    \n",
    "    # Saving tercile probs netCDF files\n",
    "    probs.to_netcdf(f'{DATADIR}/{hcst_bname}.{aggr}.tercile_probs.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb424f-d327-4905-8bd8-5016f7cf67ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ca412",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Compute deterministic and probabilistic scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0d476",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this section we will produce a set of metrics and scores consistent with the [guidance on forecast verification](https://library.wmo.int/doc_num.php?explnum_id=4886) provided by WMO. Those values can be used to describe statistics of forecast performance over a past period (the reference period defined in the variable `config`), as an indication of expectation of 'confidence' in the real-time outputs.\n",
    "\n",
    "In this notebook metrics and scores for both deterministic (anomalies) and probabilistic (probabilities of tercile categories) products, as those provided in the C3S seasonal forecast graphical products section, will be computed. Specifically:\n",
    "* Deterministic:\n",
    "    * linear temporal (Pearson) correlation\n",
    "* Probabilistic:\n",
    "    * area under the ROC curve (relative, or 'receiver', operating characteristic)\n",
    "    * RPS (ranked probability score)\n",
    "\n",
    "Values for both 1-month and 3-months aggregations will be produced and stored into netCDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509938a-0bb8-4083-9a99-ffaf93ae8592",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ddd83f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4.A. Read observations data into a xr.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b7755",
   "metadata": {},
   "source": [
    "Before any score can be calculated we will need to read into an `xr.Dataset` object the observational data from the GRIB file we downloaded in the first section of this notebook (whose name was stored in variable `obs_fname`).\n",
    "\n",
    "Similarly to what we did with the hindcast data, some minor modifications of the `xr.Dataset` object also happen here:\n",
    "\n",
    "* We rename some coordinates to align them with the names in the hindcast `xr.Dataset` and those expected by default in the `xskillscore` package we will be using for the calculation of scores.\n",
    "* We index the reanalysis data with `forecastMonth` to allow an easy like-for-like comparison of valid times.\n",
    "\n",
    "In addition to the monthly averages we retrieved from the CDS, a rolling average of 3 months is also computed to use as observational reference for the 3-months aggregation on the hindcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06c3f1ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file '/home/mohamed/EHTPIII/MODELISATION/DATA/era5_monthly_stmonth03_1993-2016.grib.5b7b6.idx' older than GRIB file\n"
     ]
    }
   ],
   "source": [
    "era5_1deg = xr.open_dataset(obs_fname, engine='cfgrib')\n",
    "\n",
    "# Renaming to match hindcast names \n",
    "era5_1deg = era5_1deg.rename({'latitude':'lat','longitude':'lon','time':'start_date'}).swap_dims({'start_date':'valid_time'})\n",
    "\n",
    "# Assign 'forecastMonth' coordinate values\n",
    "fcmonths = [mm+1 if mm>=0 else mm+13 for mm in [t.month - config['start_month'] for t in pd.to_datetime(era5_1deg.valid_time.values)] ]\n",
    "era5_1deg = era5_1deg.assign_coords(forecastMonth=('valid_time',fcmonths))\n",
    "# Drop obs values not needed (earlier than first start date) - this is useful to create well shaped 3-month aggregations from obs.\n",
    "era5_1deg = era5_1deg.where(era5_1deg.valid_time>=np.datetime64('{hcstarty}-{start_month:02d}-01'.format(**config)),drop=True)\n",
    "\n",
    "# CALCULATE 3-month AGGREGATIONS\n",
    "# NOTE rolling() assigns the label to the end of the N month period\n",
    "# NOTE care should be taken with the data available in the \"obs\" xr.Dataset so the rolling mean (over valid_time) is meaningful\n",
    "era5_1deg_3m = era5_1deg.rolling(valid_time=3).mean()\n",
    "era5_1deg_3m = era5_1deg_3m.where(era5_1deg_3m.forecastMonth>=3)\n",
    "\n",
    "# As we don't need it anymore at this stage, we can safely remove 'forecastMonth'\n",
    "era5_1deg = era5_1deg.drop('forecastMonth')\n",
    "era5_1deg_3m = era5_1deg_3m.drop('forecastMonth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda37d9-09b5-4dc3-9db3-ed106ddfb1ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e894315",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4.B. Compute deterministic scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f647452",
   "metadata": {},
   "source": [
    "The score used here is temporal correlation (Spearman rank correlation) calculated at each grid point and for each leadtime.\n",
    "\n",
    "We will start reading the netCDF files containing the anomalies we computed in section 3 of this notebook into a `xr.Dataset` object.\n",
    "\n",
    "For an easier use of the `xs.spearman_r` function a loop over the hindcast leadtimes (`forecastMonth`) has been introduced, requiring as a final step a concatenation of the values computed for each leadtime.\n",
    "\n",
    "In addition to the correlation, the p-values will be computed to allow us plotting not only the values of correlation, but also where those values are statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb128ae2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>NOTE</b>: <br>\n",
    "   The computations shown here are prepared to cater for anomalies files containing all ensemble members or files containing only an ensemble mean anomaly. The presence (or absence) of a dimension called <code>number</code> in the hindcast <code>xr.Dataset</code> object will indicate we have a full ensemble (or an ensemble mean).<br>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71b1c4bf-98c0-4235-8487-9e8c03c45e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /home/mohamed/EHTPIII/MODELISATION/DATA\n",
      "File Name: cmcc_s35_stmonth03_hindcast1993-2016_monthly.1m.anom.nc\n",
      "Full Path: /home/mohamed/EHTPIII/MODELISATION/DATA/cmcc_s35_stmonth03_hindcast1993-2016_monthly.1m.anom.nc\n"
     ]
    }
   ],
   "source": [
    "print(f'Directory: {DATADIR}')\n",
    "print(f'File Name: {hcst_bname}.{aggr}.anom.nc')\n",
    "print(f'Full Path: {DATADIR}/{hcst_bname}.{aggr}.anom.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cbd5cf6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mohamed/EHTPIII/MODELISATION/DATA/cmcc_s35_stmonth03_hindcast1993-2016_monthly.1m.anom.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/mohamed/EHTPIII/MODELISATION/DATA/cmcc_s35_stmonth03_hindcast1993-2016_monthly.1m.anom.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '5fa49117-200e-4cd5-909d-6cc5c73bb770']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown aggregation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maggr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Computing deterministic scores\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Read anomalies file\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m h \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATADIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhcst_bname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maggr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.anom.nc\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[1;32m     15\u001b[0m is_fullensemble \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdims\n\u001b[1;32m     17\u001b[0m l_corr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/api.py:611\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    600\u001b[0m     decode_cf,\n\u001b[1;32m    601\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    610\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 611\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    612\u001b[0m     filename_or_obj,\n\u001b[1;32m    613\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    616\u001b[0m )\n\u001b[1;32m    617\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    618\u001b[0m     backend_ds,\n\u001b[1;32m    619\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    630\u001b[0m )\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:649\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    630\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    647\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    648\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 649\u001b[0m     store \u001b[38;5;241m=\u001b[39m NetCDF4DataStore\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    650\u001b[0m         filename_or_obj,\n\u001b[1;32m    651\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    653\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m    654\u001b[0m         clobber\u001b[38;5;241m=\u001b[39mclobber,\n\u001b[1;32m    655\u001b[0m         diskless\u001b[38;5;241m=\u001b[39mdiskless,\n\u001b[1;32m    656\u001b[0m         persist\u001b[38;5;241m=\u001b[39mpersist,\n\u001b[1;32m    657\u001b[0m         lock\u001b[38;5;241m=\u001b[39mlock,\n\u001b[1;32m    658\u001b[0m         autoclose\u001b[38;5;241m=\u001b[39mautoclose,\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:410\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    404\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    405\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    408\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    409\u001b[0m )\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(manager, group\u001b[38;5;241m=\u001b[39mgroup, mode\u001b[38;5;241m=\u001b[39mmode, lock\u001b[38;5;241m=\u001b[39mlock, autoclose\u001b[38;5;241m=\u001b[39mautoclose)\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:357\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:419\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire()\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:413\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    414\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/MODEL/lib/python3.12/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2470\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2107\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mohamed/EHTPIII/MODELISATION/DATA/cmcc_s35_stmonth03_hindcast1993-2016_monthly.1m.anom.nc'"
     ]
    }
   ],
   "source": [
    "# Loop over aggregations\n",
    "for aggr in ['1m','3m']:\n",
    "\n",
    "    if aggr=='1m':\n",
    "        o = era5_1deg\n",
    "    elif aggr=='3m':\n",
    "        o = era5_1deg_3m\n",
    "    else:\n",
    "        raise BaseException(f'Unknown aggregation {aggr}')\n",
    "\n",
    "    # Computing deterministic scores\n",
    "\n",
    "    # Read anomalies file\n",
    "    h = xr.open_dataset(f'{DATADIR}/{hcst_bname}.{aggr}.anom.nc' )\n",
    "    is_fullensemble = 'number' in h.dims\n",
    "\n",
    "    l_corr=list()\n",
    "    l_corr_pval=list()\n",
    "\n",
    "    for this_fcmonth in h.forecastMonth.values:\n",
    "        thishcst = h.sel(forecastMonth=this_fcmonth).swap_dims({'start_date':'valid_time'})\n",
    "        thisobs = o.where(o.valid_time==thishcst.valid_time,drop=True)\n",
    "        thishcst_em = thishcst if not is_fullensemble else thishcst.mean('number')\n",
    "        l_corr.append( xs.spearman_r(thishcst_em, thisobs, dim='valid_time') )\n",
    "        l_corr_pval.append ( xs.spearman_r_p_value(thishcst_em, thisobs, dim='valid_time') )\n",
    "\n",
    "    # Concatenating (by fcmonth) correlation\n",
    "    corr=xr.concat(l_corr,dim='forecastMonth')\n",
    "    corr_pval=xr.concat(l_corr_pval,dim='forecastMonth')\n",
    "\n",
    "    # Saving to netCDF file correlation   \n",
    "    corr.to_netcdf(f'{DATADIR}/scores/{hcst_bname}.{aggr}.corr.nc')\n",
    "    corr_pval.to_netcdf(f'{DATADIR}/scores/{hcst_bname}.{aggr}.corr_pval.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074e3e6-3ecc-4888-8a5a-aeffd6bfe23f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59707591",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4.C. Compute probabilistic scores for tercile categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f0e84",
   "metadata": {},
   "source": [
    "The scores used here for probabilistic forecasts are the area under the Relative Operating Characteristic (ROC) curve, the Ranked Probability Score (RPS) and the Brier Score (BS). Note that both ROC and BS are appropriate for binary events (applied individually to each category), while RPS scores all categories together. \n",
    "\n",
    "We will now read the netCDF files containing the probabilities we computed in section 2 of this notebook into a `xr.Dataset` object. Note the variables `quantiles` and `numcategories` defined there and describing the categories for which the probabilities have been computed will be also needed here.\n",
    "\n",
    "The step to compute the probabilities for each category in the observation data for each aggregation (1-month or 3-months) is also performed here starting from the `xr.Dataset` objects read in section 3a. (`era5_1deg` and `era5_1deg_3m`).\n",
    "\n",
    "A loop over the hindcast leadtimes (`forecastMonth`) has been introduced for an easier use of the `xskillscore` functions to compute ROC curve area and RPS. This requires as a final step a concatenation of the values computed for each leadtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc7c79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>NOTE</b>: <br>\n",
    "   A couple of additional examples, both the calculation of Brier Score (BS) and the calculation of ROC Skill Score using climatology as a reference (which has a ROC value of 0.5), have been also included in the code below, but no plots will be produced for them in section 4 of the notebook.<br>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095e10f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over aggregations\n",
    "for aggr in ['1m','3m']:\n",
    "\n",
    "    if aggr=='1m':\n",
    "        o = era5_1deg\n",
    "    elif aggr=='3m':\n",
    "        o = era5_1deg_3m\n",
    "    else:\n",
    "        raise BaseException(f'Unknown aggregation {aggr}')\n",
    "\n",
    "    # Computing deterministic scores    \n",
    "    # READ hindcast probabilities file\n",
    "    probs_hcst = xr.open_dataset(f'{DATADIR}/{hcst_bname}.{aggr}.tercile_probs.nc')\n",
    "\n",
    "    l_roc=list()\n",
    "    l_rps=list()\n",
    "    l_rocss=list()\n",
    "    l_bs=list()\n",
    "    for this_fcmonth in probs_hcst.forecastMonth.values:\n",
    "        thishcst = probs_hcst.sel(forecastMonth=this_fcmonth).swap_dims({'start_date':'valid_time'})\n",
    "\n",
    "        # CALCULATE probabilities (tercile categories) from observations\n",
    "        l_probs_obs=list()\n",
    "        thiso = o.where(o.valid_time==thishcst.valid_time,drop=True)\n",
    "        for icat in range(numcategories):\n",
    "            #print(f'category={icat}')\n",
    "            o_lo,o_hi = get_thresh(icat, quantiles, thiso, dims=['valid_time'])\n",
    "            probo = 1. * np.logical_and(thiso>o_lo, thiso<=o_hi)\n",
    "            if 'quantile' in probo:\n",
    "                probo=probo.drop('quantile')\n",
    "            l_probs_obs.append(probo.assign_coords({'category':icat}))\n",
    "\n",
    "        thisobs = xr.concat(l_probs_obs, dim='category')\n",
    "\n",
    "        # Calculate the probabilistic (tercile categories) scores\n",
    "        thisroc = xr.Dataset()\n",
    "        thisrps = xr.Dataset()\n",
    "        thisrocss = xr.Dataset()\n",
    "        thisbs = xr.Dataset()\n",
    "        for var in thishcst.data_vars:\n",
    "            thisroc[var] = xs.roc(thisobs[var],thishcst[var], dim='valid_time', bin_edges=np.linspace(0,1,101))\n",
    "\n",
    "            thisrps[var] = xs.rps(thisobs[var],thishcst[var], dim='valid_time', category_edges=None, input_distributions='p')\n",
    "\n",
    "            thisrocss[var] = (thisroc[var] - 0.5) / (1. - 0.5)\n",
    "            bscat = list()\n",
    "            for cat in thisobs[var].category:\n",
    "                thisobscat = thisobs[var].sel(category=cat)\n",
    "                thishcstcat = thishcst[var].sel(category=cat)\n",
    "                bscat.append(xs.brier_score(thisobscat, thishcstcat, dim='valid_time'))\n",
    "            thisbs[var] = xr.concat(bscat,dim='category')\n",
    "\n",
    "        l_roc.append(thisroc)\n",
    "        l_rps.append(thisrps)\n",
    "        l_rocss.append(thisrocss)\n",
    "        l_bs.append(thisbs)\n",
    "\n",
    "\n",
    "    # concat roc\n",
    "    roc=xr.concat(l_roc,dim='forecastMonth')\n",
    "    # concat rps\n",
    "    rps=xr.concat(l_rps,dim='forecastMonth')\n",
    "    # concat rocss\n",
    "    rocss=xr.concat(l_rocss,dim='forecastMonth')\n",
    "    # concat bs\n",
    "    bs=xr.concat(l_bs,dim='forecastMonth')\n",
    "\n",
    "    # writing to netcdf\n",
    "    rps.to_netcdf(f'{DATADIR}/scores/{hcst_bname}.{aggr}.rps.nc')\n",
    "    bs.to_netcdf(f'{DATADIR}/scores/{hcst_bname}.{aggr}.bs.nc')\n",
    "    roc.to_netcdf(f'{DATADIR}/scores/{hcst_bname}.{aggr}.roc.nc')\n",
    "    rocss.to_netcdf(f'{DATADIR}/scores/{hcst_bname}.{aggr}.rocss.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa4629-adae-4abc-b72a-90b4070bd76c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e4fc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5. Visualize verification plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b6646",
   "metadata": {},
   "source": [
    "After we have computed some metrics and scores for our seasonal forecast data, in this section some examples of visualization will be introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671493a-278e-4ec6-9e70-772a58df4451",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>NOTE</b>: <br>\n",
    "    In the \"Introduction\" section of the <a href=\"https://confluence.ecmwf.int/display/CKB/C3S+seasonal+forecasts+verification+plots\">C3S seasonal forecasts verification plots</a> page on the Copernicus Knowledge Base you can find some information on how these metrics and scores, consistent with the <a href=\"https://library.wmo.int/doc_num.php?explnum_id=4886\">guidance on forecast verification</a> provided by WMO, describe statistics of forecast performance over a past period, as an indication of expectation of 'confidence' in the real-time outputs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600ff6b-8523-41b8-a2a3-9b5256562ff7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b5fc9-2539-40b1-bccd-25f89b4664a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Common variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5464da",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start by setting up some variables to subset the plots we will be producing in this notebook:\n",
    "\n",
    "> **`aggr`**: to specify the aggregation period (possible values: \"1m\" or \"3m\") <br>\n",
    "> **`fcmonth`**: to specify the leadtime (possible values: integer from 1 to 6) <br>\n",
    "\n",
    "Additionally variables used for labelling the plots are also defined here. It has been also included a change in the default values of the font size of the titles to improve the visual aspect of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa38104",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset of plots to be produced\n",
    "aggr  = '1m'\n",
    "fcmonth = 1\n",
    "\n",
    "# Common labels to be used in plot titles\n",
    "VARNAMES = {\n",
    "    't2m' : '2-metre temperature',\n",
    "#     'sst' : 'sea-surface temperature',\n",
    "#     'msl' : 'mean-sea-level pressure',\n",
    "#     'tp'  : 'total precipitation'\n",
    "}\n",
    "\n",
    "\n",
    "CATNAMES=['lower tercile', 'middle tercile', 'upper tercile']\n",
    "\n",
    "# Change titles font size\n",
    "plt.rc('axes', titlesize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e9b21-ad4c-43c7-af55-1e1ad155481f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41b4b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Create plot titles base information\n",
    "\n",
    "In the following variables we include some relevant information to be later used in the plot titles. Specifically:\n",
    "* Line 1: Name of the institution and forecast system\n",
    "* Line 2: Start date and valid date (with appropriate formatting depending on the aggregation, 1-month or 3-months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025046f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREPARE STRINGS for TITLES\n",
    "tit_line1 = '{institution} {name}'.format(**origin_labels)\n",
    "# tit_line2_base = rf'Start month: $\\bf{calendar.month_abbr[config[\"start_month\"]].upper()}$'\n",
    "tit_line2_base = f'Start month: {calendar.month_abbr[config[\"start_month\"]].upper()}'\n",
    "\n",
    "if aggr=='1m':\n",
    "    validmonth = config['start_month'] + (fcmonth-1)\n",
    "    validmonth = validmonth if validmonth<=12 else validmonth-12\n",
    "#     tit_line2 = tit_line2_base + rf' - Valid month: $\\bf{calendar.month_abbr[validmonth].upper()}$'\n",
    "    tit_line2 = tit_line2_base + f' - Valid month: {calendar.month_abbr[validmonth].upper()}'\n",
    "elif aggr=='3m':\n",
    "    validmonths = [vm if vm<=12 else vm-12 for vm in [config['start_month'] + (fcmonth-1) - shift for shift in range(3)]]\n",
    "    validmonths = [calendar.month_abbr[vm][0] for vm in reversed(validmonths)]\n",
    "#     tit_line2 = tit_line2_base + rf' - Valid months: $\\bf{\"\".join(validmonths)}$'\n",
    "    tit_line2 = tit_line2_base + f' - Valid months: {\"\".join(validmonths)}'\n",
    "else:\n",
    "    raise BaseException(f'Unexpected aggregation {aggr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e70e1-33e6-47d9-99bc-bdac53ddd6dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb0fa9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5.A. Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2839460",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data files\n",
    "corr = xr.open_dataset(f'{DATADIR}/scores/{hcst_bname}.{aggr}.corr.nc')\n",
    "corr_pval = xr.open_dataset(f'{DATADIR}/scores/{hcst_bname}.{aggr}.corr_pval.nc')\n",
    "# RE-ARRANGE the DATASETS longitude values for plotting purposes\n",
    "corr = corr.assign_coords(lon=(((corr.lon + 180) % 360) - 180)).sortby('lon')\n",
    "corr_pval = corr_pval.assign_coords(lon=(((corr_pval.lon + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "\n",
    "thiscorr = corr.sel(forecastMonth=fcmonth)\n",
    "thiscorrpval = corr_pval.sel(forecastMonth=fcmonth)\n",
    "\n",
    "for var in thiscorr.data_vars:\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.BORDERS, edgecolor='black', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=2.)\n",
    "    corrvalues = thiscorr[var].values \n",
    "    corrpvalvalues = thiscorrpval[var].values\n",
    "\n",
    "    if corrvalues.T.shape == (thiscorr[var].lat.size, thiscorr[var].lon.size):\n",
    "        print('Data values matrices need to be transposed')\n",
    "        corrvalues = corrvalues.T\n",
    "        corrpvalvalues = corrpvalvalues.T\n",
    "    elif corrvalues.shape == (thiscorr[var].lat.size, thiscorr[var].lon.size):\n",
    "        pass                           \n",
    "        # print('Data values matrices shapes are ok')\n",
    "    else:\n",
    "        raise BaseException(f'Unexpected data value matrix shape: {corrvalues.shape}' )\n",
    "\n",
    "\n",
    "    plt.contourf(thiscorr[var].lon,thiscorr[var].lat,corrvalues,levels=np.linspace(-1.,1.,11),cmap='RdYlBu_r')\n",
    "    cb = plt.colorbar(shrink=0.5)\n",
    "    cb.ax.set_ylabel('Correlation',fontsize=12)\n",
    "    origylim = ax.get_ylim()\n",
    "    plt.contourf(thiscorrpval[var].lon,thiscorrpval[var].lat,corrpvalvalues,levels=[0.05,np.inf],hatches=['...',None],colors='none')\n",
    "    # We need to ensure after running plt.contourf() the ylim hasn't changed\n",
    "    if ax.get_ylim()!=origylim:\n",
    "        ax.set_ylim(origylim)\n",
    "\n",
    "    plt.title(tit_line1 + f' {VARNAMES[var]}' +' (stippling where significance below 95%)\\n' + tit_line2,loc='left')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    figname = f'{DATADIR}/plots/stmonth{config[\"start_month\"]:02d}/{hcst_bname}.{aggr}.fcmonth{fcmonth}.{var}.corr.png'\n",
    "    plt.savefig(figname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df72a97-fdd0-47a3-9a73-364d2c33d3b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0e1e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5.B. Ranked Probability Score (RPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc9970",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ scores xr.Datasets\n",
    "rps = xr.open_dataset(f'{DATADIR}/scores/{hcst_bname}.{aggr}.rps.nc')\n",
    "# RE-ARRANGE the DATASETS longitude values for plotting purposes\n",
    "rps = rps.assign_coords(lon=(((rps.lon + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "thisrps = rps.sel(forecastMonth=fcmonth)\n",
    "\n",
    "for var in thisrps.data_vars:\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.BORDERS, edgecolor='black', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=2.)\n",
    "    avalues = thisrps[var].values\n",
    "    cs = plt.contourf(thisrps[var].lon,thisrps[var].lat,avalues,levels=np.linspace(0.,0.5,11),cmap='YlGn_r', extend='max')\n",
    "    cs.cmap.set_under('purple')\n",
    "    cb = plt.colorbar(shrink=0.5)\n",
    "    cb.ax.set_ylabel('RPS',fontsize=12)\n",
    "    plt.title(tit_line1 + f' {VARNAMES[var]}' + ' (tercile categories)\\n' + tit_line2,loc='left')\n",
    "    plt.tight_layout()  \n",
    "    figname = f'{DATADIR}/plots/stmonth{config[\"start_month\"]:02d}/{hcst_bname}.{aggr}.fcmonth{fcmonth}.{var}.rps.png'\n",
    "    plt.savefig(figname)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2436b87-c5ce-4ee7-b6d0-0a3c2437062c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae8404",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5.C. Area under Relative Operating Characteristic (ROC) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223a873",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ scores xr.Datasets\n",
    "roc = xr.open_dataset(f'{DATADIR}/scores/{hcst_bname}.{aggr}.roc.nc')\n",
    "# RE-ARRANGE the DATASETS longitude values for plotting purposes\n",
    "roc = roc.assign_coords(lon=(((roc.lon + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "thisroc = roc.sel(forecastMonth=fcmonth)\n",
    "\n",
    "for var in thisroc.data_vars:\n",
    "    for icat in thisroc.category.values:\n",
    "        fig = plt.figure(figsize=(18,10))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        ax.add_feature(cfeature.BORDERS, edgecolor='black', linewidth=0.5)\n",
    "        ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=2.)\n",
    "        avalues = thisroc.sel(category=icat)[var].values\n",
    "        cs = plt.contourf(thisroc[var].lon,thisroc[var].lat,avalues,levels=np.linspace(0.5,1.,6),cmap='YlGn', extend='min')\n",
    "        cs.cmap.set_under('lightgray')\n",
    "        cb = plt.colorbar(shrink=0.5)\n",
    "        cb.ax.set_ylabel('Area under ROC curve',fontsize=12)\n",
    "        plt.title(tit_line1 + f' {VARNAMES[var]}' + f' ({CATNAMES[icat]})\\n' + tit_line2, loc='left')\n",
    "        plt.tight_layout()  \n",
    "        figname = f'{DATADIR}/plots/stmonth{config[\"start_month\"]:02d}/{hcst_bname}.{aggr}.fcmonth{fcmonth}.{var}.category{icat}.roc.png'\n",
    "        plt.savefig(figname)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c5f94-1a66-4782-9ac5-730f3dbb33d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb75e7-0370-4ba5-a158-3b87061f04d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "key-messages-to-take-home"
    ]
   },
   "source": [
    "## Key Messages to Take Home üìå\n",
    " \n",
    "- Seasonal forecast anomalies are defined as the difference of a given hindcast to the model climate, computed as the average over all the available members.\n",
    "\n",
    "- Probabilities for tercile categories are defined as the proportion of members in a given hindcast lying within each one of the categories bounded by the terciles computed from all available members in the reference period.\n",
    "\n",
    "- Specific verification metrics and scores are to be computed for both deterministic (anomalies) and probabilistic (probabilities of tercile categories).\n",
    "\n",
    "- All computed metrics and scores must be spatially visualized.\n",
    "\n",
    "- Information on how these metrics and scores work as indicators of 'confidence' of the real-time forecast can be found in the \"Introduction\" section of the <a href=\"https://confluence.ecmwf.int/display/CKB/C3S+seasonal+forecasts+verification+plots\">C3S seasonal forecasts verification plots</a> page on the Copernicus Knowledge Base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852dfaf-01b5-4f94-8004-fb823bea9f9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "MODEL",
   "language": "python",
   "name": "model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "notebook_group": "seasonal-forecasts-tutorial",
  "notebook_name": "sf-verification",
  "notebook_title": "Seasonal Forecast Verification",
  "tags": [
   "multidimensional arrays",
   "hindcast",
   "anomalies",
   "deterministic and probabilistic scores",
   "statistics of forecast performance"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
